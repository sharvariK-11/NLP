{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDJMUHAxSr9rJVPcYqFzov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharvariK-11/NLP/blob/main/Bag%20of%20Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKk2f8XN2wxv"
      },
      "source": [
        "### BAG OF WORDS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rr9MiXy23lf"
      },
      "source": [
        "In sentiment analysis, its imp to know which word is imp, bow has quite a disadv. wrt that.\n",
        "# BINARY BOW\n",
        "Basically in each sentence we assign either n(1) or 0 to each word i.e meaningful words, not \"the,this,for,is etc\"{this is called SENTENCE LOWERING'} in that sentence if it is present ntimes or absent resp.\n",
        " eg:\n",
        " He is good boy boy.\n",
        " She is good girl.\n",
        "\n",
        " WORDS AS VECTORS:\n",
        "                    good    boy    girl\n",
        "sentence 1:          1       2       0\n",
        "sentence 1:          1       0       1\n",
        "\n",
        " for sentiment analysis use BOW, for large dataset useword to back"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zWR8aFU5eoS"
      },
      "source": [
        "### Implementing BOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KchvlZDr9DFm"
      },
      "source": [
        "[ ] IS A LIST, split() creates a list of all those words which are seperated by spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrnhYx40-PoS",
        "outputId": "3c3dce47-2ba5-441e-f10f-694c0d656df8"
      },
      "source": [
        "! pip install nltk\n",
        "import nltk"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5pO63ErAkZ6",
        "outputId": "fb8e893a-8d1d-44be-a11e-0e34514e1835"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkr_cf7CBEsW"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXlzuZhzBQcQ"
      },
      "source": [
        "paragraph =  \"\"\"I have three visions for India. In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds. \n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,the French, the Dutch, all of them came and looted us, took over what was ours. \n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\"\"\" "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k67zhW3oBl1Q"
      },
      "source": [
        "### CLEANING TEXTS\n",
        " ( lowering of text, stemming,lammetization,removing punctuations)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVvirCOpCFzR"
      },
      "source": [
        "import re\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "wordnet= WordNetLemmatizer()\n",
        "corpus=[]\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbH9oJFXKK9H",
        "outputId": "1c8b5b7e-420a-40ab-bdf6-bf9ed70c6956"
      },
      "source": [
        ""
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNo0DRIRD6Aa"
      },
      "source": [
        "re= regular expression used for cleaning text. the corpus = [ ] prepares list of all sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGXBYK2vEeZo",
        "outputId": "cf9821a3-5bc9-4fdb-ad55-bff063757a86"
      },
      "source": [
        "sentences"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXHXLrSZE3Gp"
      },
      "source": [
        "for i in range(len(sentences)):\n",
        "  replace=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "  low=replace.lower()\n",
        "  wordlist = low.split()\n",
        "  lemma = [wordnet.lemmatize(word) for word in wordlist if not word in set(stopwords.words('english'))]\n",
        "  lemma = ' '.join(lemma)\n",
        "  corpus.append(lemma)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9yOD0LgK8CU"
      },
      "source": [
        "lemma1 creates a list consisting of all lemmatized words for each sentence\n",
        "''.join(lemma) joins all those words and creates modified sentence,this happens for each sentence.\n",
        "corpus initially was empty list, corpus.append(lemma) adds lemma to the list i.e. it creates list consisting of all modified sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGchx7_RFugp"
      },
      "source": [
        "re.sub([^a],b,c) means for all c ( here, sentences) replace everything except a by b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmYYBskRGd-Y",
        "outputId": "31ee2ecb-c6ed-4d8e-e292-109c2383ce58"
      },
      "source": [
        "wordlist"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we', 'have', 'not', 'conquered', 'anyone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ta-93MJ0Jqm7",
        "outputId": "c3672c60-1ef8-48fe-cd96-0b781036d17d"
      },
      "source": [
        "lemma"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'conquered anyone'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8v9GTbQJuGa",
        "outputId": "5f8e90ed-4b12-4c4a-a957-58db50118a41"
      },
      "source": [
        "corpus"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three vision india',\n",
              " 'year history people world come invaded u captured land conquered mind',\n",
              " 'alexander onwards greek turk mogul portuguese british french dutch came looted u took',\n",
              " 'yet done nation',\n",
              " 'conquered anyone',\n",
              " 'conquered anyone',\n",
              " 'three vision india',\n",
              " 'year history people world come invaded u captured land conquered mind',\n",
              " 'alexander onwards greek turk mogul portuguese british french dutch came looted u took',\n",
              " 'yet done nation',\n",
              " 'conquered anyone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGGNrFvyMVtT"
      },
      "source": [
        "### creating BAG OF WORDS MODEL\n",
        "CountVectorizer is a text pre processing library,used to create bow matrix i.e sentences to vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiJAXSjkNHMg"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "matrix= cv.fit_transform(corpus).toarray()\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzC1mudLNuA6"
      },
      "source": [
        "matrix creates a matrix out of corpus, as corpus consists of sentences as members of list, each sent is converted to [1 0 0 ....] arrays so on to make a matrix finally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m6cTC6EOO9f",
        "outputId": "8018c823-3c3d-4a56-dd71-cf0090b3399f"
      },
      "source": [
        "matrix"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "        0, 1, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 0, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "        0, 1, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    }
  ]
}